#!/usr/bin/env python3
"""
Rayé›†ç¾¤å†…éƒ¨vLLMæ¨ç†API - å¼ é‡å¹¶è¡Œç‰ˆæœ¬
å°†7Bæ¨¡å‹åˆ‡ç‰‡åˆ°2å¼ GPUä¸Š
"""

import ray
import asyncio
import logging
from typing import List

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@ray.remote(num_gpus=2)  # ä½¿ç”¨2å¼ å®Œæ•´GPUè¿›è¡Œå¼ é‡å¹¶è¡Œ
class VLLMTensorParallelActor:
    """vLLMå¼ é‡å¹¶è¡Œæ¨ç†actor - æ¨¡å‹åˆ‡ç‰‡åˆ°2å¼ GPU"""
    
    def __init__(self, model_path: str):
        self.model_path = model_path
        self.llm = None
        
    def initialize(self):
        """åˆå§‹åŒ–vLLMå¼•æ“ï¼ˆå¼ é‡å¹¶è¡Œï¼‰"""
        from vllm import LLM, SamplingParams
        
        # å…³é”®ï¼štensor_parallel_size=2 å°†æ¨¡å‹åˆ‡ç‰‡åˆ°2å¼ GPU
        self.llm = LLM(
            model=self.model_path,
            tensor_parallel_size=2,  # ğŸ”¥ æ¨¡å‹åˆ‡ç‰‡åˆ°2å¼ GPU
            gpu_memory_utilization=0.3,
            trust_remote_code=True,
            max_model_len=8192,
        )
        
        self.sampling_params = SamplingParams(
            max_tokens=1024,  # æ¯æ¬¡æ¨ç†æœ€å¤šç”Ÿæˆ1024ä¸ªtoken
            temperature=0.1,
            top_p=0.9,
            top_k=50,
            repetition_penalty=1.0,
            presence_penalty=0.0,
            frequency_penalty=0.0,
            stop=["<|im_end|>", "\n\n"]
        )
        
        logger.info("âœ… æ¨¡å‹å·²åˆ‡ç‰‡åˆ°2å¼ GPU")
        return "Tensor parallel model loaded"
    
    def generate(self, prompt: str) -> str:
        """å•ä¸ªæ¨ç†"""
        outputs = self.llm.generate([prompt], self.sampling_params)
        return outputs[0].outputs[0].text.strip()
    
    def batch_generate(self, prompts: List[str]) -> List[str]:
        """æ‰¹é‡æ¨ç†"""
        outputs = self.llm.generate(prompts, self.sampling_params)
        return [output.outputs[0].text.strip() for output in outputs]

@ray.remote
class RayVLLMService:
    """ç®€åŒ–çš„Ray vLLMæœåŠ¡ç®¡ç†å™¨"""
    
    def __init__(self, model_path: str):
        self.model_path = model_path
        self.actor = None
        
    async def start_service(self):
        """å¯åŠ¨æœåŠ¡"""
        logger.info(f"å¯åŠ¨å¼ é‡å¹¶è¡ŒvLLMæœåŠ¡: {self.model_path}")
        
        # åˆ›å»ºå•ä¸ªå¼ é‡å¹¶è¡Œactor
        self.actor = VLLMTensorParallelActor.remote(self.model_path)
        
        # âœ… ä¿®æ­£ï¼šåªç”¨ awaitï¼Œä¸ç”¨ ray.get()
        result = await self.actor.initialize.remote()
        logger.info(f"åˆå§‹åŒ–ç»“æœ: {result}")
        logger.info("âœ… æœåŠ¡å¯åŠ¨æˆåŠŸ")
        return True
    
    async def generate(self, prompt: str) -> str:
        """å•ä¸ªæ¨ç†"""
        # âœ… ä¿®æ­£ï¼šåªç”¨ await
        return await self.actor.generate.remote(prompt)
    
    async def batch_generate(self, prompts: List[str]) -> List[str]:
        """æ‰¹é‡æ¨ç†"""
        # âœ… ä¿®æ­£ï¼šåªç”¨ await
        return await self.actor.batch_generate.remote(prompts)

# å…¨å±€æœåŠ¡å®ä¾‹
_service = None

async def get_service(model_path: str = "/data/home/scyb224/Workspace/LLMs/Qwen2.5-7B-Instruct"):
    """è·å–æœåŠ¡å®ä¾‹"""
    global _service
    if _service is None:
        _service = RayVLLMService.remote(model_path)
        await _service.start_service.remote()
    return _service

# ç®€åŒ–çš„APIå‡½æ•°
async def ray_generate(prompt: str) -> str:
    """æ¨ç†å•ä¸ªprompt"""
    service = await get_service()
    return await service.generate.remote(prompt)

async def ray_batch_generate(prompts: List[str]) -> List[str]:
    """æ‰¹é‡æ¨ç†"""
    service = await get_service()
    return await service.batch_generate.remote(prompts)

async def start_service_only():
    """åªå¯åŠ¨æœåŠ¡ï¼Œä¸è¿›å…¥æ— é™å¾ªç¯"""
    if not ray.is_initialized():
        ray.init()
    
    # åˆå§‹åŒ–æœåŠ¡
    await get_service()
    print("âœ… Ray vLLMæœåŠ¡å¯åŠ¨å®Œæˆï¼Œå¯ä»¥æ¥å—è¯·æ±‚")

# å¯åŠ¨è„šæœ¬
async def main():
    """å¯åŠ¨æœåŠ¡å¹¶ä¿æŒè¿è¡Œ"""
    await start_service_only()
    
    # è¿è¡Œä¸€ä¸ªç®€å•æµ‹è¯•
    response = await ray_generate("Hello, how are you?")
    print(f"æµ‹è¯•å“åº”: {response}")
    
    # ä¿æŒè¿è¡Œ
    print("ğŸ”„ æœåŠ¡ä¿æŒè¿è¡Œä¸­... (Ctrl+C åœæ­¢)")
    try:
        while True:
            await asyncio.sleep(10)
    except KeyboardInterrupt:
        print("Service stopped")

if __name__ == "__main__":
    asyncio.run(main())
